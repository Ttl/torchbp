#!/usr/bin/env python
import torch
from torch.testing._internal.common_utils import TestCase
from torch.testing._internal.optests import opcheck
import unittest
import torchbp
from torch import Tensor
from typing import Tuple
import torch.nn.functional as F

class TestEntropy(TestCase):
    def sample_inputs(self, device, *, requires_grad=False, dtype=torch.complex64):
        def make_tensor(size, dtype=dtype):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            return x

        def make_nondiff_tensor(size, dtype=dtype):
            return torch.randn(size, device=device, requires_grad=False, dtype=dtype)

        args = {
            'img': make_tensor((3, 3), dtype=dtype)
        }
        return [args]

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_ref(self):
        samples = self.sample_inputs("cuda", requires_grad=True)
        for sample in samples:
            sample_cpu = {k:sample[k].detach().cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            for k in sample.keys():
                if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad:
                    sample_cpu[k].requires_grad = True

            res_gpu = torchbp.ops.entropy(sample["img"])
            res_gpu.backward()
            grads_gpu = [sample[k].cpu() for k in sample.keys() if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad]

            res_cpu = torchbp.util.entropy(sample_cpu["img"])
            res_cpu.backward()
            grads_cpu = [sample_cpu[k] for k in sample_cpu.keys() if isinstance(sample_cpu[k], torch.Tensor) and sample_cpu[k].requires_grad]
            torch.testing.assert_close(grads_cpu, grads_gpu)
            torch.testing.assert_close(res_gpu.cpu(), res_cpu)

class TestPolarInterpLinear(TestCase):
    def sample_inputs(self, device, *, requires_grad=False, dtype=torch.float32):
        def make_tensor(size, dtype=dtype):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            return x

        def make_nondiff_tensor(size, dtype=dtype):
            return torch.randn(size, device=device, requires_grad=False, dtype=dtype)

        complex_dtype = torch.complex64 if dtype == torch.float32 else torch.complex128
        nbatch = 2
        grid_polar = {"r": (10, 20), "theta": (-1, 1), "nr": 2, "ntheta": 2}
        grid_polar_new = {"r": (12, 18), "theta": (-0.8, 0.8), "nr": 3, "ntheta": 3}
        dorigin = 0.1*make_tensor((nbatch, 3), dtype=dtype)
        # TODO: Remove this when Z-axis interpolation is supported.
        dorigin[:,2] = 0
        args = {
            'img': make_tensor((nbatch, grid_polar["nr"], grid_polar["ntheta"]), dtype=complex_dtype),
            'dorigin': dorigin,
            'grid_polar': grid_polar,
            'fc': 6e9,
            'rotation': 0,
            'grid_polar_new': grid_polar_new,
        }
        return [args]

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_cpu_and_gpu_grad(self):
        samples = self.sample_inputs("cuda", requires_grad=True)
        for sample in samples:
            sample_cpu = {k:sample[k].detach().cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            for k in sample.keys():
                if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad:
                    sample_cpu[k].requires_grad = True

            res_gpu = torchbp.ops.polar_interp_linear(**sample)
            loss_gpu = torch.mean(torch.abs(res_gpu))
            loss_gpu.backward()
            grads_gpu = [sample[k].cpu() for k in sample.keys() if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad]

            res_cpu = torchbp.ops.polar_interp_linear(**sample_cpu)
            loss_cpu = torch.mean(torch.abs(res_cpu))
            loss_cpu.backward()
            grads_cpu = [sample_cpu[k] for k in sample_cpu.keys() if isinstance(sample_cpu[k], torch.Tensor) and sample_cpu[k].requires_grad]
            torch.testing.assert_close(grads_cpu, grads_gpu, atol=1e-3, rtol=1e-2)

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_cpu_and_gpu(self):
        samples = self.sample_inputs("cuda")
        for sample in samples:
            res_gpu = torchbp.ops.polar_interp_linear(**sample).cpu()
            sample_cpu = {k:sample[k].cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            res_cpu = torchbp.ops.polar_interp_linear(**sample_cpu)
            torch.testing.assert_close(res_cpu, res_gpu, rtol=5e-4, atol=5e-4)

    def _test_gradients(self, device, dtype=torch.float32):
        samples = self.sample_inputs(device, requires_grad=True, dtype=dtype)
        eps = 5e-4 if dtype == torch.float32 else 1e-4
        rtol = 0.15 if dtype == torch.float32 else 0.05
        for args in samples:
            torch.autograd.gradcheck(
                    torchbp.ops.polar_interp_linear,
                    list(args.values()),
                    eps=eps, # This test is very sensitive to eps
                    rtol=rtol, # Also to rtol
                    )

    def test_gradients_cpu(self):
        self._test_gradients("cpu")
        self._test_gradients("cpu", dtype=torch.float64)

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_gradients_cuda(self):
        self._test_gradients("cuda")

class TestPolarToCartLinear(TestCase):
    def sample_inputs(self, device, *, requires_grad=False, dtype=torch.float32):
        def make_tensor(size, dtype=dtype):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            return x

        def make_nondiff_tensor(size, dtype=dtype):
            return torch.randn(size, device=device, requires_grad=False, dtype=dtype)

        complex_dtype = torch.complex64 if dtype == torch.float32 else torch.complex128
        nbatch = 2
        grid_polar = {"r": (10, 20), "theta": (-1, 1), "nr": 2, "ntheta": 2}
        grid_cart = {"x": (12, 18), "y": (-5, 5), "nx": 3, "ny": 3}
        #TODO: make origin differentiable
        origin = 0.1*make_nondiff_tensor((nbatch, 3), dtype=dtype)
        args = {
            'img': make_tensor((nbatch, grid_polar["nr"], grid_polar["ntheta"]), dtype=complex_dtype),
            'origin': origin,
            'grid_polar': grid_polar,
            'grid_cart': grid_cart,
            'fc': 6e9,
            'rotation': 0,
            'polar_interp': False
        }
        return [args]

    #@unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    @unittest.skip
    def test_cpu_and_gpu_grad(self):
        samples = self.sample_inputs("cuda", requires_grad=True)
        for sample in samples:
            sample_cpu = {k:sample[k].detach().cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            for k in sample.keys():
                if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad:
                    sample_cpu[k].requires_grad = True

            res_gpu = torchbp.ops.polar_to_cart_linear(**sample)
            loss_gpu = torch.mean(torch.abs(res_gpu))
            loss_gpu.backward()
            grads_gpu = [sample[k].cpu() for k in sample.keys() if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad]

            res_cpu = torchbp.ops.polar_to_cart_linear(**sample_cpu)
            loss_cpu = torch.mean(torch.abs(res_cpu))
            loss_cpu.backward()
            grads_cpu = [sample_cpu[k] for k in sample_cpu.keys() if isinstance(sample_cpu[k], torch.Tensor) and sample_cpu[k].requires_grad]
            torch.testing.assert_close(grads_cpu, grads_gpu, atol=1e-3, rtol=1e-2)

    #@unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    @unittest.skip
    def test_cpu_and_gpu(self):
        samples = self.sample_inputs("cuda")
        for sample in samples:
            res_gpu = torchbp.ops.polar_to_cart_linear(**sample).cpu()
            sample_cpu = {k:sample[k].cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            res_cpu = torchbp.ops.polar_to_cart_linear(**sample_cpu)
            torch.testing.assert_close(res_cpu, res_gpu)

    def _test_gradients(self, device, dtype=torch.float32):
        samples = self.sample_inputs(device, requires_grad=True, dtype=dtype)
        eps = 5e-4 if dtype == torch.float32 else 1e-4
        rtol = 0.15 if dtype == torch.float32 else 0.05
        for args in samples:
            torch.autograd.gradcheck(
                    torchbp.ops.polar_to_cart_linear,
                    list(args.values()),
                    eps=eps, # This test is very sensitive to eps
                    rtol=rtol, # Also to rtol
                    )

    @unittest.skip
    def test_gradients_cpu(self):
        self._test_gradients("cpu")
        self._test_gradients("cpu", dtype=torch.float64)

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_gradients_cuda(self):
        self._test_gradients("cuda")

class TestPolarToCartBicubic(TestCase):
    def sample_inputs(self, device, *, requires_grad=False, dtype=torch.float32):
        def make_tensor(size, dtype=dtype):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            return x

        def make_nondiff_tensor(size, dtype=dtype):
            return torch.randn(size, device=device, requires_grad=False, dtype=dtype)

        complex_dtype = torch.complex64 if dtype == torch.float32 else torch.complex128
        nbatch = 2
        grid_polar = {"r": (10, 20), "theta": (-1, 1), "nr": 3, "ntheta": 3}
        grid_cart = {"x": (12, 18), "y": (-5, 5), "nx": 3, "ny": 3}
        #TODO: make origin differentiable
        origin = 0.1*make_nondiff_tensor((nbatch, 3), dtype=dtype)
        args = {
            'img': make_tensor((nbatch, grid_polar["nr"], grid_polar["ntheta"]), dtype=complex_dtype),
            'origin': origin,
            'grid_polar': grid_polar,
            'grid_cart': grid_cart,
            'fc': 6e9,
            'rotation': 0,
            'polar_interp': False
        }
        return [args]

    #@unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    @unittest.skip
    def test_cpu_and_gpu_grad(self):
        samples = self.sample_inputs("cuda", requires_grad=True)
        for sample in samples:
            sample_cpu = {k:sample[k].detach().cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            for k in sample.keys():
                if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad:
                    sample_cpu[k].requires_grad = True

            res_gpu = torchbp.ops.polar_to_cart_bicubic(**sample)
            loss_gpu = torch.mean(torch.abs(res_gpu))
            loss_gpu.backward()
            grads_gpu = [sample[k].cpu() for k in sample.keys() if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad]

            res_cpu = torchbp.ops.polar_to_cart_bicubic(**sample_cpu)
            loss_cpu = torch.mean(torch.abs(res_cpu))
            loss_cpu.backward()
            grads_cpu = [sample_cpu[k] for k in sample_cpu.keys() if isinstance(sample_cpu[k], torch.Tensor) and sample_cpu[k].requires_grad]
            torch.testing.assert_close(grads_cpu, grads_gpu, atol=1e-3, rtol=1e-2)

    #@unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    @unittest.skip
    def test_cpu_and_gpu(self):
        samples = self.sample_inputs("cuda")
        for sample in samples:
            res_gpu = torchbp.ops.polar_to_cart_bicubic(**sample).cpu()
            sample_cpu = {k:sample[k].cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            res_cpu = torchbp.ops.polar_to_cart_bicubic(**sample_cpu)
            torch.testing.assert_close(res_cpu, res_gpu)

    def _test_gradients(self, device, dtype=torch.float32):
        samples = self.sample_inputs(device, requires_grad=True, dtype=dtype)
        eps = 7e-4 if dtype == torch.float32 else 1e-4
        rtol = 0.2 if dtype == torch.float32 else 0.05
        for args in samples:
            torch.autograd.gradcheck(
                    torchbp.ops.polar_to_cart_bicubic,
                    list(args.values()),
                    eps=eps, # This test is very sensitive to eps
                    rtol=rtol, # Also to rtol
                    )

    @unittest.skip
    def test_gradients_cpu(self):
        self._test_gradients("cpu")
        self._test_gradients("cpu", dtype=torch.float64)

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_gradients_cuda(self):
        self._test_gradients("cuda")

class TestBackprojectionPolar(TestCase):
    def sample_inputs(self, device, *, requires_grad=False):
        def make_tensor(size, dtype=torch.float32):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            return x

        # Make sure that scene is in view
        def make_pos_tensor(size, dtype=torch.float32):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            x = x - torch.max(x[:,0]) - 2
            return x

        def make_nondiff_tensor(size, dtype=torch.float32):
            return torch.randn(size, device=device, requires_grad=False, dtype=dtype)

        nbatch = 2
        sweeps = 2
        sweep_samples = 64
        grid = {"r": (1, 10), "theta": (-0.9, 0.9), "nr": 4, "ntheta": 4}
        args = {
            'data': make_tensor((nbatch, sweeps, sweep_samples), dtype=torch.complex64),
            'grid': grid,
            'fc': 6e9,
            'r_res': 0.15,
            'pos': make_pos_tensor((nbatch, sweeps, 3), dtype=torch.float32),
            'd0': 0.2,
        }
        return [args]

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_cpu_and_gpu(self):
        samples = self.sample_inputs("cuda")
        for sample in samples:
            res_gpu = torchbp.ops.backprojection_polar_2d(**sample).cpu()
            sample_cpu = {k:sample[k].cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            res_cpu = torchbp.ops.backprojection_polar_2d(**sample_cpu)
            torch.testing.assert_close(res_cpu, res_gpu, atol=1e-3, rtol=1e-2)

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_cpu_and_gpu_grad(self):
        samples = self.sample_inputs("cuda", requires_grad=True)
        for sample in samples:
            sample_cpu = {k:sample[k].detach().cpu() if isinstance(sample[k], torch.Tensor) else sample[k] for k in sample.keys()}
            for k in sample.keys():
                if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad:
                    sample_cpu[k].requires_grad = True

            res_gpu = torchbp.ops.backprojection_polar_2d(**sample)
            loss_gpu = torch.mean(torch.abs(res_gpu))
            loss_gpu.backward()
            grads_gpu = [sample[k].cpu() for k in sample.keys() if isinstance(sample[k], torch.Tensor) and sample[k].requires_grad]

            res_cpu = torchbp.ops.backprojection_polar_2d(**sample_cpu)
            loss_cpu = torch.mean(torch.abs(res_cpu))
            loss_cpu.backward()
            grads_cpu = [sample_cpu[k] for k in sample_cpu.keys() if isinstance(sample_cpu[k], torch.Tensor) and sample_cpu[k].requires_grad]
            torch.testing.assert_close(grads_cpu, grads_gpu, atol=1e-3, rtol=1e-2)

    def _test_gradients(self, device):
        samples = self.sample_inputs(device, requires_grad=True)
        for args in samples:
            torch.autograd.gradcheck(
                    torchbp.ops.backprojection_polar_2d,
                    list(args.values()),
                    eps=5e-4, # This test is very sensitive to eps
                    rtol=0.2, # Also to rtol
                    atol=0.05
                    )

    def test_gradients_cpu(self):
        self._test_gradients("cpu")

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_gradients_cuda(self):
        self._test_gradients("cuda")

class TestBackprojectionCart(TestCase):

    def sample_inputs(self, device, *, requires_grad=False):
        def make_tensor(size, dtype=torch.float32):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            return x

        # Make sure that scene is in view
        def make_pos_tensor(size, dtype=torch.float32):
            x = torch.randn(size, device=device, requires_grad=requires_grad, dtype=dtype)
            x = x - torch.max(x[:,0]) - 2
            return x

        def make_nondiff_tensor(size, dtype=torch.float32):
            return torch.randn(size, device=device, requires_grad=False, dtype=dtype)

        nbatch = 2
        sweeps = 2
        sweep_samples = 128
        grid = {"x": (2, 10), "y": (-5, 5), "nx": 4, "ny": 4}
        args = {
            'data': make_tensor((nbatch, sweeps, sweep_samples), dtype=torch.complex64),
            'grid': grid,
            'fc': 6e9,
            'r_res': 0.15,
            'pos': make_pos_tensor((nbatch, sweeps, 3), dtype=torch.float32),
            'beamwidth': 3.14,
            'd0': 0.2,
        }
        return [args]

    def _test_gradients(self, device):
        samples = self.sample_inputs(device, requires_grad=True)
        for args in samples:
            torch.autograd.gradcheck(
                    torchbp.ops.backprojection_cart_2d,
                    list(args.values()),
                    eps=5e-4, # This test is very sensitive to eps
                    rtol=0.2, # Also to rtol
                    atol=0.05

                    )

    #def test_gradients_cpu(self):
    #    self._test_gradients("cpu")

    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    def test_gradients_cuda(self):
        self._test_gradients("cuda")

    #def _opcheck(self, device):
    #    # Use opcheck to check for incorrect usage of operator registration APIs
    #    samples = self.sample_inputs(device, requires_grad=True)
    #    samples.extend(self.sample_inputs(device, requires_grad=False))
    #    for args in samples:
    #        opcheck(torch.ops.torchbp.backprojection_cart_2d, list(args.values()))

    #def test_opcheck_cpu(self):
    #    self._opcheck("cpu")

    #@unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
    #def test_opcheck_cuda(self):
    #    self._opcheck("cuda")

if __name__ == "__main__":
    unittest.main()
